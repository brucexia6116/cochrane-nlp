{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Load and train a Keras model for character-level language modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/home/ebanner/.anaconda/envs/py27/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "with open('model.json', 'rb') as f:\n",
    "    model = model_from_json(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "abstracts_char_lm = pickle.load(open('data.p', 'rb'))\n",
    "\n",
    "abstracts_padded = abstracts_char_lm['abstracts_padded']\n",
    "embeddings = abstracts_char_lm['embeddings']\n",
    "char2idx, idx2char = abstracts_char_lm['char2idx'], abstracts_char_lm['idx2char']\n",
    "\n",
    "num_abstracts, maxlen = len(abstracts_padded), len(abstracts_padded[0])\n",
    "vocab_dim, num_chars = embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split up into Train and Validation\n",
    "\n",
    "Due to efficiency reasons, the train and the test set are represented as matrixes which are the same size of the entire dataset, but are zero everywhere except in the entries which correspond to their examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "fold = KFold(len(abstracts_padded), n_folds=20)\n",
    "p = iter(fold)\n",
    "\n",
    "train_idxs, val_idxs = next(p)\n",
    "\n",
    "xs_train, xs_val = np.zeros_like(abstracts_padded), np.zeros_like(abstracts_padded)\n",
    "xs_train[train_idxs], xs_val[val_idxs] = abstracts_padded[train_idxs], abstracts_padded[val_idxs]\n",
    "\n",
    "def example_generator(xs, vocab_dim, num_steps=None):\n",
    "    \"\"\"Yields the next x, y pair\n",
    "    \n",
    "    Assume that xs have already been zero-padded to the same length\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_steps : int\n",
    "        It's too expensive during validation to run along the entire abstract, so only go for a subset\n",
    "    \n",
    "    \"\"\"\n",
    "    num_abstracts = len(xs)\n",
    "    \n",
    "    num_steps = len(xs[0]) if not num_steps else num_steps # default to going through the entire abstract\n",
    "    \n",
    "    for t in range(num_steps-1):\n",
    "        y = np.zeros([num_abstracts, vocab_dim]) # desired one-hot output vectors\n",
    "        non_zero = xs[:, t+1] != MASK_VALUE # leave the masked characters at zero\n",
    "        y[np.arange(num_abstracts)[non_zero], xs[:, t+1][non_zero]] = 1 # y is xs at next time step\n",
    "\n",
    "        yield xs[:, [t]], y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Over Several Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "perplexity: 1.00235134719\n",
      "200\n",
      "perplexity: 1.00234515794\n",
      "300\n",
      "perplexity: 1.00232643806\n",
      "400\n",
      "perplexity: 1.00233795268\n",
      "500\n",
      "perplexity: 1.00232749024\n",
      "600\n",
      "perplexity: 1.0023197974\n",
      "700\n",
      "perplexity: 1.00232757907\n",
      "800\n",
      "perplexity: 1.00232197065\n",
      "900\n",
      "perplexity: 1.00231353319\n",
      "1000\n",
      "perplexity: 1.00230797697\n",
      "1100\n",
      "perplexity: 1.00229870776\n",
      "1200\n",
      "perplexity: 1.00228256127\n",
      "1300\n",
      "perplexity: 1.00227999775\n",
      "1400\n",
      "perplexity: 1.00227425166\n",
      "1500\n",
      "perplexity: 1.00226986723\n",
      "1600\n",
      "perplexity: 1.00226773478\n",
      "1700\n",
      "perplexity: 1.00225946331\n",
      "1800\n",
      "perplexity: 1.00226136006\n",
      "1900\n",
      "perplexity: 1.00225965619\n",
      "2000\n",
      "perplexity: 1.0022572015\n",
      "2100\n",
      "perplexity: 1.00225221964\n",
      "2200\n",
      "perplexity: 1.00224208477\n",
      "2300\n",
      "perplexity: 1.00223381244\n",
      "2400\n",
      "perplexity: 1.00222856809\n",
      "2500\n",
      "perplexity: 1.00222787095\n",
      "2600\n",
      "perplexity: 1.00220979479\n",
      "2700\n",
      "perplexity: 1.00218227942\n",
      "2800\n",
      "perplexity: 1.00217041284\n",
      "2900\n",
      "perplexity: 1.00214036475\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "MASK_VALUE = 0\n",
    "\n",
    "num_vals = len(xs_val)\n",
    "\n",
    "for _ in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.reset_states() # start off at zero initialization\n",
    "    \n",
    "    train_example = example_generator(xs_train, vocab_dim)\n",
    "\n",
    "    for t in range(maxlen-1):\n",
    "        if not t % 100:\n",
    "            print t\n",
    "            \n",
    "        x, y = next(train_example)\n",
    "        model.train_on_batch(x, y)\n",
    "        \n",
    "        if not t % 100 and not t == 0:\n",
    "            val_examples = example_generator(xs_val, vocab_dim, num_steps=100)\n",
    "            \n",
    "            saved_states = copy.deepcopy(model.layers[1].states) # save the states in the lstm layer\n",
    "            model.reset_states()\n",
    "            perplexity = np.mean([2**model.evaluate(x, y, batch_size=num_abstracts, verbose=0) for x, y in val_examples])\n",
    "            print 'perplexity: {}'.format(perplexity)\n",
    "            \n",
    "        model.layers[1].states = saved_states # reset the states so training can pick up where it left off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Sample from the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from support import sample_sentences\n",
    "\n",
    "X = sample_sentences(model, num_abstracts, vocab_dim, char2idx, idx2char, sent_len=10)\n",
    "\n",
    "for sentence_idxed in X:\n",
    "    print ''.join([idx2char[idx] for idx in sentence_idxed])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
