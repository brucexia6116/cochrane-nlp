{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%store -r cnn_model\n",
    "\n",
    "dataset = cnn_model['dataset']\n",
    "\n",
    "abstracts_padded = cnn_model['abstracts_padded']\n",
    "label_map, ys = cnn_model['label_map'], cnn_model['ys']\n",
    "labels = [i for gender, i in label_map.items()]\n",
    "num_classes = cnn_model['num_classes']\n",
    "\n",
    "embeddings = cnn_model['embeddings']\n",
    "word_dim = cnn_model['word_dim']\n",
    "word2idx, idx2word = cnn_model['word2idx'], cnn_model['idx2word']\n",
    "maxlen = cnn_model['maxlen']\n",
    "vocab_size = cnn_model['vocab_size']\n",
    "num_train = cnn_model['num_train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "fold = KFold(len(abstracts_padded), n_folds=5, shuffle=True)\n",
    "p = iter(fold)\n",
    "\n",
    "train_idxs, val_idxs = next(p)\n",
    "\n",
    "X_train, ys_train = abstracts_padded[train_idxs], ys[train_idxs]\n",
    "X_val, ys_val = abstracts_padded[val_idxs], ys[val_idxs]\n",
    "\n",
    "num_train, num_val = len(X_train), len(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_filter = 10\n",
    "filter_length = 2\n",
    "hidden_dims = 32\n",
    "nb_epoch = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 487, 10)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=word_dim, weights=[embeddings], input_length=maxlen,\n",
    "                   trainable=False))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        activation='relu'))\n",
    "model.add(MaxPooling1D(pool_length=maxlen-1)) # non-maximum suppression\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "model.layers[3].input_shape # ensure non-maximum suppression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Balanced Minibatch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def labelidx_generator(ys):\n",
    "    \"\"\"Generate a dict of labels to their indices\"\"\"\n",
    "\n",
    "    for label in labels:\n",
    "        idxs = np.argwhere(ys == label).flatten()\n",
    "        yield (label, idxs)\n",
    "\n",
    "def batch_generator(ys, batch_size, balanced=True):\n",
    "    \"\"\"Yield successive batches for training\n",
    "    \n",
    "    This generator is not meant to be exhausted, but rather called by next().\n",
    "    \n",
    "    Each batch has batch_size/num_classes number of examples from each class\n",
    "    \n",
    "    \"\"\"\n",
    "    assert not batch_size % num_classes\n",
    "    \n",
    "    labels_idxs = dict(labelidx_generator(ys))\n",
    "    \n",
    "    while True:\n",
    "        idxs_lists = [np.random.choice(label_idxs, size=batch_size/num_classes) for label, label_idxs in labels_idxs.items()]\n",
    "        idxs = [idx for idxs_list in idxs_lists for idx in idxs_list] # flatten list\n",
    "        \n",
    "        yield idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 33\n",
    "\n",
    "example = batch_generator(ys_train, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(0.8047752380371094, dtype=float32)]\n",
      "Validation accuracy 0.6\n",
      "[array(0.8439961671829224, dtype=float32)]\n",
      "[array(0.7106217741966248, dtype=float32)]\n",
      "[array(0.7445381879806519, dtype=float32)]\n",
      "[array(0.7547301054000854, dtype=float32)]\n",
      "[array(0.7134566903114319, dtype=float32)]\n",
      "[array(0.7483267784118652, dtype=float32)]\n",
      "[array(0.7668272256851196, dtype=float32)]\n",
      "[array(0.7335414290428162, dtype=float32)]\n",
      "[array(0.719879686832428, dtype=float32)]\n",
      "[array(0.7679944038391113, dtype=float32)]\n",
      "Validation accuracy 0.633333333333\n",
      "[array(0.7509118914604187, dtype=float32)]\n",
      "[array(0.676609456539154, dtype=float32)]\n",
      "[array(0.6686001420021057, dtype=float32)]\n",
      "[array(0.7070923447608948, dtype=float32)]\n",
      "[array(0.763827919960022, dtype=float32)]\n",
      "[array(0.6709403395652771, dtype=float32)]\n",
      "[array(0.7338182330131531, dtype=float32)]\n",
      "[array(0.7468081116676331, dtype=float32)]\n",
      "[array(0.7250101566314697, dtype=float32)]\n",
      "[array(0.7146375775337219, dtype=float32)]\n",
      "Validation accuracy 0.6\n",
      "[array(0.6788461208343506, dtype=float32)]\n",
      "[array(0.6841240525245667, dtype=float32)]\n",
      "[array(0.639314591884613, dtype=float32)]\n",
      "[array(0.6093370914459229, dtype=float32)]\n",
      "[array(0.6562624573707581, dtype=float32)]\n",
      "[array(0.6245619654655457, dtype=float32)]\n",
      "[array(0.7133053541183472, dtype=float32)]\n",
      "[array(0.5946618318557739, dtype=float32)]\n",
      "[array(0.6624155044555664, dtype=float32)]\n",
      "[array(0.5996310710906982, dtype=float32)]\n",
      "Validation accuracy 0.7\n",
      "[array(0.5764298439025879, dtype=float32)]\n",
      "[array(0.6083287000656128, dtype=float32)]\n",
      "[array(0.6095251441001892, dtype=float32)]\n",
      "[array(0.5688447952270508, dtype=float32)]\n",
      "[array(0.6455633044242859, dtype=float32)]\n",
      "[array(0.7004349231719971, dtype=float32)]\n",
      "[array(0.6238211989402771, dtype=float32)]\n",
      "[array(0.6163691282272339, dtype=float32)]\n",
      "[array(0.6304541826248169, dtype=float32)]\n",
      "[array(0.6192513108253479, dtype=float32)]\n",
      "Validation accuracy 0.633333333333\n",
      "[array(0.5918684601783752, dtype=float32)]\n",
      "[array(0.5916029214859009, dtype=float32)]\n",
      "[array(0.6743625998497009, dtype=float32)]\n",
      "[array(0.6622384190559387, dtype=float32)]\n",
      "[array(0.6201432347297668, dtype=float32)]\n",
      "[array(0.5330650210380554, dtype=float32)]\n",
      "[array(0.6028556227684021, dtype=float32)]\n",
      "[array(0.5983214974403381, dtype=float32)]\n",
      "[array(0.5658161044120789, dtype=float32)]\n",
      "[array(0.5443106889724731, dtype=float32)]\n",
      "Validation accuracy 0.633333333333\n",
      "[array(0.6321992874145508, dtype=float32)]\n",
      "[array(0.5198028087615967, dtype=float32)]\n",
      "[array(0.5430512428283691, dtype=float32)]\n",
      "[array(0.5860905051231384, dtype=float32)]\n",
      "[array(0.5247858762741089, dtype=float32)]\n",
      "[array(0.48086193203926086, dtype=float32)]\n",
      "[array(0.5939813256263733, dtype=float32)]\n",
      "[array(0.6120602488517761, dtype=float32)]\n",
      "[array(0.4533384144306183, dtype=float32)]\n",
      "[array(0.5363790392875671, dtype=float32)]\n",
      "Validation accuracy 0.7\n",
      "[array(0.4632419943809509, dtype=float32)]\n",
      "[array(0.5409749746322632, dtype=float32)]\n",
      "[array(0.4761716425418854, dtype=float32)]\n",
      "[array(0.5924009680747986, dtype=float32)]\n",
      "[array(0.5689430832862854, dtype=float32)]\n",
      "[array(0.48697084188461304, dtype=float32)]\n",
      "[array(0.5248067378997803, dtype=float32)]\n",
      "[array(0.5099008083343506, dtype=float32)]\n",
      "[array(0.4903073310852051, dtype=float32)]\n",
      "[array(0.5238820314407349, dtype=float32)]\n",
      "Validation accuracy 0.7\n",
      "[array(0.46917393803596497, dtype=float32)]\n",
      "[array(0.4651365578174591, dtype=float32)]\n",
      "[array(0.4708462357521057, dtype=float32)]\n",
      "[array(0.47418782114982605, dtype=float32)]\n",
      "[array(0.5115305781364441, dtype=float32)]\n",
      "[array(0.5528025031089783, dtype=float32)]\n",
      "[array(0.4985325038433075, dtype=float32)]\n",
      "[array(0.6138623356819153, dtype=float32)]\n",
      "[array(0.48908743262290955, dtype=float32)]\n",
      "[array(0.44362908601760864, dtype=float32)]\n",
      "Validation accuracy 0.7\n",
      "[array(0.5793847441673279, dtype=float32)]\n",
      "[array(0.4436412751674652, dtype=float32)]\n",
      "[array(0.32565680146217346, dtype=float32)]\n",
      "[array(0.4605117738246918, dtype=float32)]\n",
      "[array(0.43449896574020386, dtype=float32)]\n",
      "[array(0.41076669096946716, dtype=float32)]\n",
      "[array(0.5349216461181641, dtype=float32)]\n",
      "[array(0.42691752314567566, dtype=float32)]\n",
      "[array(0.3891027271747589, dtype=float32)]\n",
      "[array(0.3987851142883301, dtype=float32)]\n",
      "Validation accuracy 0.733333333333\n",
      "[array(0.38188567757606506, dtype=float32)]\n",
      "[array(0.4476225972175598, dtype=float32)]\n",
      "[array(0.4557460844516754, dtype=float32)]\n",
      "[array(0.3957289755344391, dtype=float32)]\n",
      "[array(0.42568129301071167, dtype=float32)]\n",
      "[array(0.4339480996131897, dtype=float32)]\n",
      "[array(0.4214344024658203, dtype=float32)]\n",
      "[array(0.4775518476963043, dtype=float32)]\n",
      "[array(0.35881873965263367, dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    batch = next(example)\n",
    "    \n",
    "    X = X_train[batch]\n",
    "    ys = np.zeros([batch_size, num_classes])\n",
    "    ys[np.arange(batch_size), ys_train[batch]] = 1\n",
    "\n",
    "    print model.train_on_batch(X, ys)\n",
    "    \n",
    "    if not i % 10:\n",
    "        predictions = model.predict(X_val)\n",
    "\n",
    "        ysval_block = np.zeros([num_val, num_classes])\n",
    "        ysval_block[np.arange(num_val), ys_val] = 1\n",
    "\n",
    "        print 'Validation accuracy', np.mean(predictions.argmax(axis=1) == ys_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASUAAAElCAYAAACiZ/R3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fee8109e510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = model.predict(X_val)\n",
    "\n",
    "ysval_block = np.zeros([num_val, num_classes])\n",
    "ysval_block[np.arange(num_val), ys_val] = 1\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(ys_val, predictions.argmax(axis=1))\n",
    "\n",
    "from support import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(cm, label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Bigrams Which Filters Fire on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D-Cycloserine, a partial agonist at the glycine site of the N-methyl-D-aspartate receptor, has demonstrated inconsistent efficacy for negative and cognitive symptoms of schizophrenia. The strongest evidence for efficacy has come from studies using D-cycloserine at a dose of 50 mg/day added to conventional antipsychotics in trials of 8 weeks duration or less. \\n                 To assess the efficacy for negative symptoms and cognitive impairment of D-cycloserine augmentation of conventional antipsychotics in a 6-month trial. \\n                 Fifty-five schizophrenia patients with prominent negative symptoms, treated with conventional antipsychotics, were randomly assigned to treatment with D-cycloserine 50 mg/day or placebo for 6 months in a double-blind, parallel group design. \\n                 Twenty-six subjects completed the 6-month trial; drop-out rates did not differ between treatment groups. D-Cycloserine treatment did not differ from placebo treatment on any primary outcome measure at 8 or 24 weeks, including response of negative symptoms and performance on a cognitive battery. Serum D-cycloserine concentrations did not correlate with response of negative symptoms. \\n                 D-Cycloserine did not exhibit therapeutic effects in this trial, possibly reflecting the high drop-out rate, a narrow range of therapeutic serum concentrations, a modest magnitude of therapeutic effect for the selected outcome measures, or loss of efficacy over time. Because D-cycloserine is a partial agonist with relatively low affinity for the glycine site, the magnitude of potential therapeutic effect may be smaller than that achieved by the higher-affinity full agonists, glycine and D-serine.'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.abstract.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Both **********\n",
      "0.682580284712 may be\n",
      "0.455139355498 D-cycloserine is\n",
      "0.358263281175 possibly reflecting\n",
      "0.332385171115 reflecting the\n",
      "0.323503592954 the N-methyl-D-aspartate\n",
      "0.311441936647 strongest evidence\n",
      "0.288976079914 in a\n",
      "0.288976079914 in a\n",
      "0.268276217463 in this\n",
      "0.267926909413 the 6-month\n",
      "\n",
      "0.290560964054 patients with\n",
      "0.2895958744 augmentation of\n",
      "0.242680950261 loss of\n",
      "0.215280116817 weeks duration\n",
      "0.184264144981 any primary\n",
      "0.180453482067 subjects completed\n",
      "0.16669675686 high drop-out\n",
      "0.163752142039 drop-out rates\n",
      "0.161911823728 correlate with\n",
      "0.147880670635 drop-out rate\n",
      "\n",
      "0.841647503351 for 6\n",
      "0.794228559572 at 8\n",
      "0.7294093399 of 8\n",
      "0.654612362823 or 24\n",
      "0.376032314638 24 weeks\n",
      "0.353352431588 8 weeks\n",
      "0.323700663088 efficacy over\n",
      "0.321491230391 D-cycloserine at\n",
      "0.297801809103 measure at\n",
      "0.287726979601 agonist at\n",
      "\n",
      "0.306370556418 cognitive impairment\n",
      "0.296201840438 a cognitive\n",
      "0.216173065762 Fifty-five schizophrenia\n",
      "0.210686960461 Serum D-cycloserine\n",
      "0.209224780346 cognitive battery\n",
      "0.180027705341 any primary\n",
      "0.178995116109 performance on\n",
      "0.16992830929 and cognitive\n",
      "0.16992830929 and cognitive\n",
      "0.162243627128 magnitude of\n",
      "\n",
      "0.441025416646 outcome measure\n",
      "0.401782485317 may be\n",
      "0.302607366184 outcome measures\n",
      "0.294154753404 inconsistent efficacy\n",
      "0.289868181528 efficacy has\n",
      "0.28709260375 To assess\n",
      "0.286296341525 in this\n",
      "0.28336527207 therapeutic effect\n",
      "0.28336527207 therapeutic effect\n",
      "0.278108314373 of potential\n",
      "\n",
      "0.423198172057 loss of\n",
      "0.398712498771 partial agonist\n",
      "0.398712498771 partial agonist\n",
      "0.275016617303 a modest\n",
      "0.257668519827 effects in\n",
      "0.253431122006 demonstrated inconsistent\n",
      "0.241684771396 exhibit therapeutic\n",
      "0.235097972604 impairment of\n",
      "0.220024480818 that achieved\n",
      "0.215373911923 full agonists\n",
      "\n",
      "0.407931531018 with prominent\n",
      "0.284356890502 in this\n",
      "0.280717687198 at the\n",
      "0.249712209929 , the\n",
      "0.244919815249 . Because\n",
      "0.233941463167 reflecting the\n",
      "0.233802274821 . Fifty-five\n",
      "0.220350347577 in trials\n",
      "0.209611911232 by the\n",
      "0.20652984681 . The\n",
      "\n",
      "0.154202339354 N-methyl-D-aspartate receptor\n",
      "0.0841593057153 D-cycloserine augmentation\n",
      "0.0754518492148 partial agonist\n",
      "0.0754518492148 partial agonist\n",
      "0.0729771775428 therapeutic serum\n",
      "0.0724631908133 conventional antipsychotics\n",
      "0.0724631908133 conventional antipsychotics\n",
      "0.0724631908133 conventional antipsychotics\n",
      "0.0711176976574 Fifty-five schizophrenia\n",
      "0.0663955503801 <MASK> D-Cycloserine\n",
      "\n",
      "0.240313945639 cognitive symptoms\n",
      "0.238248753566 cognitive battery\n",
      "0.193959357216 cognitive impairment\n",
      "0.163618448937 N-methyl-D-aspartate receptor\n",
      "0.151568247122 negative symptoms\n",
      "0.151568247122 negative symptoms\n",
      "0.151568247122 negative symptoms\n",
      "0.151568247122 negative symptoms\n",
      "0.140118808681 of schizophrenia\n",
      "0.122905277696 a cognitive\n",
      "\n",
      "0.333145357628 cognitive impairment\n",
      "0.307261639049 Fifty-five schizophrenia\n",
      "0.266056657649 cognitive battery\n",
      "0.265492042913 cognitive symptoms\n",
      "0.245010768518 a dose\n",
      "0.191541500525 a cognitive\n",
      "0.163761677116 schizophrenia patients\n",
      "0.155031072235 conventional antipsychotics\n",
      "0.155031072235 conventional antipsychotics\n",
      "0.155031072235 conventional antipsychotics\n",
      "\n",
      "********** Female **********\n",
      "0.530781145511 deceased is\n",
      "0.503871160123 can be\n",
      "0.461666206787 reduction is\n",
      "0.338357643272 considered in\n",
      "0.323283681449 to be\n",
      "0.309625524414 seen in\n",
      "0.288976079914 in a\n",
      "0.288099148215 and shows\n",
      "0.255129952572 the cervix\n",
      "0.245489928765 the phase\n",
      "\n",
      "1.03645491267 women with\n",
      "0.681266431602 cervical cancer\n",
      "0.589726726984 cervix and\n",
      "0.551285920077 cancer metastatic\n",
      "0.441750569972 cycles concurrent\n",
      "0.394098362185 those deceased\n",
      "0.388104295118 pelvic para-aortic\n",
      "0.377533062138 late toxicity\n",
      "0.317495743195 carcinoma of\n",
      "0.290954408848 cohorts of\n",
      "\n",
      "1.03860203931 ( 2\n",
      "1.03860203931 ( 2\n",
      "1.03860203931 ( 2\n",
      "0.814396132657 in 6\n",
      "0.699563184324 in 7\n",
      "0.622603578499 Grade three\n",
      "0.553836401402 and 15\n",
      "0.552113326798 for six\n",
      "0.530285061943 and four\n",
      "0.481409036 of three\n",
      "\n",
      "0.165987484754 decrease late\n",
      "0.156478276365 maximum tolerated\n",
      "0.156478276365 maximum tolerated\n",
      "0.121723967539 seen in\n",
      "0.117730638649 at nine\n",
      "0.109140456223 at eight\n",
      "0.0975246610102 was seen\n",
      "0.0973015683985 each .\n",
      "0.0967008418432 those deceased\n",
      "0.0890139190906 maximum dose\n",
      "\n",
      "0.502382678791 considered in\n",
      "0.403364839591 of toxicity\n",
      "0.391910207398 for this\n",
      "0.382800443579 can be\n",
      "0.370674608837 described MTD\n",
      "0.36107179185 warrants validation\n",
      "0.358205361147 prospective cohort\n",
      "0.343963531917 shows a\n",
      "0.329828773623 historical data\n",
      "0.322383874064 evaluated at\n",
      "\n",
      "0.611123343561 toxicity of\n",
      "0.575882707275 toxicity and\n",
      "0.494062406365 toxicity was\n",
      "0.453539712378 cisplatin chemotherapy\n",
      "0.390535204715 neutropenia in\n",
      "0.306650375501 chemotherapy concurrent\n",
      "0.306650375501 chemotherapy concurrent\n",
      "0.279870683062 cisplatin combination\n",
      "0.27679688587 cisplatin 40\n",
      "0.273881868585 irradiation can\n",
      "\n",
      "0.766222642125 in women\n",
      "0.379252360008 phase II\n",
      "0.379252360008 phase II\n",
      "0.361669619077 phase I/II\n",
      "0.346895061421 and in\n",
      "0.343540892133 . Central\n",
      "0.339775698248 in the\n",
      "0.284516081709 in 68.4\n",
      "0.280717687198 at the\n",
      "0.280717687198 at the\n",
      "\n",
      "0.556977472317 cervical cancer\n",
      "0.451876734696 cancer metastatic\n",
      "0.317498223716 in women\n",
      "0.297372296108 the cervix\n",
      "0.261956972234 pelvic para-aortic\n",
      "0.193911322112 field pelvic\n",
      "0.158997479879 para-aortic node\n",
      "0.155587002732 with cervical\n",
      "0.133203070687 para-aortic nodes\n",
      "0.0863035816636 para-aortic irradiation\n",
      "\n",
      "0.129447059788 four gastrointestinal\n",
      "0.0991725043392 para-aortic irradiation\n",
      "0.0961786011077 pelvic para-aortic\n",
      "0.0842736656391 cervical cancer\n",
      "0.0840717945171 para-aortic node\n",
      "0.0695754715415 para-aortic nodes\n",
      "0.0675604549849 deceased is\n",
      "0.0564205532618 the cervix\n",
      "0.0551325135292 node metastases\n",
      "0.0422655366804 <MASK> To\n",
      "\n",
      "0.396344976173 those deceased\n",
      "0.3332236285 radiation dose\n",
      "0.333029501153 tolerated dose\n",
      "0.333029501153 tolerated dose\n",
      "0.279677236468 field radiation\n",
      "0.279677236468 field radiation\n",
      "0.254473918286 escalating doses\n",
      "0.241217361447 field irradiation\n",
      "0.240431135754 Central radiation\n",
      "0.192282866138 deceased is\n",
      "\n",
      "********** Male **********\n",
      "0.739259662455 important consideration\n",
      "0.467838790978 ) is\n",
      "0.467298619421 an important\n",
      "0.452612691596 related to\n",
      "0.407921516672 Toxicity is\n",
      "0.388897343574 An acute\n",
      "0.386458177419 associated with\n",
      "0.3379536975 would improve\n",
      "0.335718947286 during treatment\n",
      "0.335718947286 during treatment\n",
      "\n",
      "0.477894528727 prostate cancer\n",
      "0.477894528727 prostate cancer\n",
      "0.477894528727 prostate cancer\n",
      "0.463728052854 cycles of\n",
      "0.374270948719 high-risk prostate\n",
      "0.307173272234 compared with\n",
      "0.304104884569 nonmetastatic prostate\n",
      "0.300001773641 outcomes with\n",
      "0.29804176533 cancer patients\n",
      "0.295228264211 weeks of\n",
      "\n",
      "1.06672887275 Grade 3\n",
      "0.971174109318 and 3\n",
      "0.971174109318 and 3\n",
      "0.944809970087 At 2\n",
      "0.935957000089 at 2\n",
      "0.922344471118 Arm 2\n",
      "0.922344471118 Arm 2\n",
      "0.922344471118 Arm 2\n",
      "0.922344471118 Arm 2\n",
      "0.922344471118 Arm 2\n",
      "\n",
      "0.306226857261 Statistically significant\n",
      "0.195648071081 observed in\n",
      "0.178757457598 20-100 ng/mL\n",
      "0.177774403139 noted in\n",
      "0.164796857822 prostate-specific antigen\n",
      "0.164796857822 prostate-specific antigen\n",
      "0.160407071848 noted during\n",
      "0.150115532897 improve disease\n",
      "0.130110700334 increases in\n",
      "0.130026290995 occurred in\n",
      "\n",
      "0.594711375295 to study\n",
      "0.491677494246 acceptable toxicity\n",
      "0.467210902567 undertaken to\n",
      "0.392586998628 long-term toxicity\n",
      "0.392586998628 long-term toxicity\n",
      "0.368175391498 thromboembolic toxicity\n",
      "0.362780135043 The toxicity\n",
      "0.352486361752 performed .\n",
      "0.342797546562 determine whether\n",
      "0.316859554915 important consideration\n",
      "\n",
      "0.749332882648 toxicity profiles\n",
      "0.57920340393 toxicity during\n",
      "0.568260377136 toxicity compared\n",
      "0.556875763519 toxicity .\n",
      "0.55192089152 toxicity analysis\n",
      "0.504328730189 hematologic toxicity\n",
      "0.494062406365 toxicity was\n",
      "0.494062406365 toxicity was\n",
      "0.426981213317 toxicity (\n",
      "0.426981213317 toxicity (\n",
      "\n",
      "0.339775698248 in the\n",
      "0.331013800432 in hematologic\n",
      "0.310568603261 and the\n",
      "0.296453645657 . High-risk\n",
      "0.267103798549 and gastrointestinal\n",
      "0.253253399535 . Of\n",
      "0.25306534069 , leading\n",
      "0.243336683954 observed in\n",
      "0.237981230547 of high-risk\n",
      "0.233164127008 not genitourinary\n",
      "\n",
      "1.07310264208 nonmetastatic prostate\n",
      "0.937948525513 for prostate\n",
      "0.85217298907 prostate cancer\n",
      "0.85217298907 prostate cancer\n",
      "0.85217298907 prostate cancer\n",
      "0.84272664559 high-risk prostate\n",
      "0.794352026197 Long-term androgen\n",
      "0.53584953196 androgen suppression\n",
      "0.419090972682 prostate-specific antigen\n",
      "0.419090972682 prostate-specific antigen\n",
      "\n",
      "0.478173032412 prostate-specific antigen\n",
      "0.478173032412 prostate-specific antigen\n",
      "0.291626055766 Radiation Therapy\n",
      "0.291626055766 Radiation Therapy\n",
      "0.286501505063 nonmetastatic prostate\n",
      "0.253280049436 antigen level\n",
      "0.24720852376 Therapy Oncology\n",
      "0.24720852376 Therapy Oncology\n",
      "0.241094151339 , prostate-specific\n",
      "0.234739091944 ( prostate-specific\n",
      "\n",
      "0.360085825624 myelogenous leukemia\n",
      "0.332434130759 antigen level\n",
      "0.312845885826 myelodysplasia/acute myelogenous\n",
      "0.296317552028 infection occurred\n",
      "0.26096210873 neutropenic infection\n",
      "0.224541038026 Group trial\n",
      "0.199186672867 disease outcomes\n",
      "0.188864926255 Therapy Oncology\n",
      "0.188864926255 Therapy Oncology\n",
      "0.188621617934 plus radiotherapy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filters = model.layers[2].W.eval()\n",
    "filters = np.squeeze(filters)\n",
    "filters = [filter.T for filter in filters]\n",
    "\n",
    "def activation_generator(filter, abstract):\n",
    "    for w1, w2 in zip(abstract, abstract[1:]):\n",
    "        yield np.sum(embeddings[[w1, w2]] * filter), (w1, w2)\n",
    "        \n",
    "def activations_generator(filters, abstract):\n",
    "    for filter in filters:\n",
    "        yield list(activation_generator(filter, abstract))\n",
    "        \n",
    "def show_activations(filters, abstract):        \n",
    "    activations = list(activations_generator(filters, abstract))\n",
    "\n",
    "    for activation in activations:\n",
    "        for score, (w1, w2) in sorted(activation, reverse=True)[:10]:\n",
    "            print score, idx2word[w1], idx2word[w2]\n",
    "\n",
    "        print\n",
    "        \n",
    "for gender, idx in zip(['Both', 'Female', 'Male'], [0, 50, 100]):\n",
    "    print '*'*10, gender, '*'*10\n",
    "    show_activations(filters, abstracts_padded[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
