{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `word2vec`\n",
    "\n",
    "- Pull out `word2vec` vectors for every word in the corpus and make an embedding matrix\n",
    "- Transform each abstract into a list of indexes into the embedding matrix\n",
    "- Save it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%store -r abstracts_targets_collapsed\n",
    "\n",
    "dataset = abstracts_targets_collapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take Only a Minimal Subset of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def examples_generator(dataset, target='gender'):\n",
    "    \"\"\"Generate indexes into dataset for one training example for each class\"\"\"\n",
    "    \n",
    "    labels = dataset[target].unique()\n",
    "\n",
    "    for label in labels:\n",
    "        yield dataset[dataset[target] == label].iloc[0].name\n",
    "\n",
    "dataset = dataset.iloc[list(examples_generator(dataset))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abstracts = dataset.abstract\n",
    "abstracts = abstracts.map(lambda abstract: abstract.decode('utf-8')) # decode character encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put All Unique Words in Abstracts into a Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "words = set()\n",
    "for abstract in abstracts:\n",
    "    for word in word_tokenize(abstract.decode('utf-8')):\n",
    "        words.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-Trained Pubmed `word2vec` Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec.load_word2vec_format('/home/ebanner/Research/data/word2vec/PubMed-w2v.bin', binary=True)  # C binary format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate word $\\rightarrow$ index Mapping for Mini-Embedding Matrix\n",
    "\n",
    "Additionally add in the `<MASK>` entry as the zeroth element in the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2idx = {word: idx for idx, word in enumerate(words)} # ignore unknown words for now\n",
    "\n",
    "word2idx = {word: idx+1 for word, idx in word2idx.items()} # push every word down one spot to make room for the mask\n",
    "word2idx['<<<MASK>>>'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Mini-Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def w2v_generator(word2idx):\n",
    "    \"\"\"Generate word2vec vectors for each word\n",
    "    \n",
    "    It's assumed that every word in word2idx contains in word2vec\n",
    "    \n",
    "    \"\"\"\n",
    "    for word, _ in sorted(word2idx.items(), key=operator.itemgetter(1)):\n",
    "        if word not in model:\n",
    "            yield np.zeros(model.vector_size) # just yield all zeros for OOV words (including the mask)\n",
    "        else:\n",
    "            yield model[word]\n",
    "        \n",
    "W = np.array(list(w2v_generator(word2idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Each Abstract to a List of Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def abstracts2idxs_generator(abstracts):\n",
    "    for i, abstract in enumerate(abstracts):\n",
    "        yield list(abstract2idxs(abstract))\n",
    "    \n",
    "def abstract2idxs(abstract):\n",
    "    for word in word_tokenize(abstract):\n",
    "        yield word2idx[word]\n",
    "        \n",
    "abstracts_idxed = list(abstracts2idxs_generator(abstracts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the Abstracts to a Fixed Length\n",
    "\n",
    "The maximum value is the length of the longest abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "maxlen = max(len(abstract_idxed) for abstract_idxed in abstracts_idxed)\n",
    "\n",
    "abstracts_padded = sequence.pad_sequences(abstracts_idxed, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> D-Cycloserine , a partial agonist at the glycine site of the N-methyl-D-aspartate receptor , has demonstrated inconsistent efficacy for negative and cognitive symptoms of schizophrenia . The strongest evidence for efficacy has come from studies using D-cycloserine at a dose of 50 mg/day added to conventional antipsychotics in trials of 8 weeks duration or less . To assess the efficacy for negative symptoms and cognitive impairment of D-cycloserine augmentation of conventional antipsychotics in a 6-month trial . Fifty-five schizophrenia patients with prominent negative symptoms , treated with conventional antipsychotics , were randomly assigned to treatment with D-cycloserine 50 mg/day or placebo for 6 months in a double-blind , parallel group design . Twenty-six subjects completed the 6-month trial ; drop-out rates did not differ between treatment groups . D-Cycloserine treatment did not differ from placebo treatment on any primary outcome measure at 8 or 24 weeks , including response of negative symptoms and performance on a cognitive battery . Serum D-cycloserine concentrations did not correlate with response of negative symptoms . D-Cycloserine did not exhibit therapeutic effects in this trial , possibly reflecting the high drop-out rate , a narrow range of therapeutic serum concentrations , a modest magnitude of therapeutic effect for the selected outcome measures , or loss of efficacy over time . Because D-cycloserine is a partial agonist with relatively low affinity for the glycine site , the magnitude of potential therapeutic effect may be smaller than that achieved by the higher-affinity full agonists , glycine and D-serine .\n",
      "\n",
      "<<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> <<<MASK>>> To determine the maximum tolerated dose ( MTD ) of weekly paclitaxel and cisplatin chemotherapy concurrent with extended field irradiation in women with cervical cancer metastatic to the para-aortic nodes . Patients with carcinoma of the cervix and histologically documented para-aortic node metastases were eligible for this phase I/II trial . Chemotherapy agents were administered weekly concurrent with extended field radiation with escalating doses of paclitaxel from 30-50 mg/m ( 2 ) in each of three cohorts of three patients each . A phase II cohort was then evaluated at the selected maximum tolerated dose ( MTD ) . The MTD was determined to be cisplatin 40 mg/m ( 2 ) ( maximum dose of 70 mg ) and paclitaxel 40 mg/m ( 2 ) administered weekly for six cycles concurrent with extended field radiation therapy . There were 19 evaluable patients for the phase II analysis of toxicity and efficacy . Grade three and four gastrointestinal toxicity was seen in 6 and neutropenia in 7 . Radiation therapy was successfully completed in 36.8 % of patients at eight weeks and in 68.4 % of patients at nine weeks , with a median time to completion was 56 days . A total of 27 evaluable patients were enrolled , twelve are dead ( mean survival of those deceased is 25 months ) , and 15 ( 56 % ) are alive , and have been followed for a mean of 48 months ( range 25-68 ; median of 46 months ) . Paclitaxel and cisplatin combination chemotherapy concurrent with extended field pelvic para-aortic irradiation can be administered at the described MTD and shows a higher than previously reported disease-free survival in relation to historical data . The 56 % survival to date , and 50 % estimated 48 month survival , warrants validation in a larger prospective cohort . Central radiation dose reduction is being considered in the next trial to decrease late toxicity of regimen .\n",
      "\n",
      "Long-term androgen suppression plus radiotherapy ( AS+RT ) is standard treatment of high-risk prostate cancer . A randomized trial , Radiation Therapy Oncology Group trial 9902 , was undertaken to determine whether adjuvant chemotherapy with paclitaxel , estramustine , and etoposide ( TEE ) plus AS+RT would improve disease outcomes with acceptable toxicity . High-risk ( prostate-specific antigen 20-100 ng/mL and Gleason score > or=7 ; or Stage T2 or greater , Gleason score 8 , prostate-specific antigen level < 100 ng/mL ) nonmetastatic prostate cancer patients were randomized to AS+RT ( Arm 1 ) vs. AS+RT plus four cycles of TEE ( Arm 2 ) . TEE was delivered 4 weeks after RT . AS continued for 2 years for both treatment arms . RT began after 8 weeks of AS began . The Radiation Therapy Oncology Group 9902 trial opened January 11 , 2000 . Excess thromboembolic toxicity was noted , leading to study closure October 4 , 2004 . A total of 397 patients were accrued , and the data for 381 were analyzable . An acute and long-term toxicity analysis was performed . The worst overall toxicities during treatment were increased for Arm 2 . Of the 192 patients , 136 ( 71 % ) on Arm 2 had RTOG Grade 3 or greater toxicity compared with 70 ( 37 % ) of 189 patients on Arm 1 . Statistically significant increases in hematologic toxicity ( p < 0.0001 ) and gastrointestinal toxicity ( p = 0.017 ) but not genitourinary toxicity ( p = 0.07 ) were noted during treatment . Two Grade 5 complications related to neutropenic infection occurred in Arm 2 . Three cases of myelodysplasia/acute myelogenous leukemia were noted in Arm 2 . At 2 and 3 years after therapy completion , excess long-term toxicity was not observed in Arm 2 . TEE was associated with significantly increased toxicity during treatment . The toxicity profiles did not differ at 2 and 3 years after therapy . Toxicity is an important consideration in the design of trials using adjuvant chemotherapy for prostate cancer .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "for abstract_padded in abstracts_padded:\n",
    "    print ' '.join([idx2word[idx] for idx in abstract_padded])\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Following:\n",
    "\n",
    "- Abstracts represented as indices into embedding matrix\n",
    "- Embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'cnn_model' (dict)\n"
     ]
    }
   ],
   "source": [
    "genders = dataset.gender\n",
    "ys = genders.map({gender: i for i, gender in enumerate(genders)})\n",
    "\n",
    "cnn_model = {\n",
    "            'dataset': dataset,\n",
    "            'abstracts_padded': abstracts_padded,\n",
    "            'ys': ys,\n",
    "            'embeddings': W,\n",
    "            'word_dim': model.vector_size,\n",
    "            'maxlen': maxlen,\n",
    "            'vocab_size': len(W),\n",
    "            'num_classes': len(genders),\n",
    "            'num_train': len(dataset),\n",
    "            'word2idx': word2idx,\n",
    "            'idx2word': idx2word,\n",
    "}\n",
    "\n",
    "%store cnn_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
