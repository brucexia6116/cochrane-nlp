Addictive Behaviors 35 (2010) 660–666

Contents lists available at ScienceDirect

Addictive Behaviors

Improving the quality of reporting alcohol outcome studies: Effects of the
CONSORT statement
Benjamin O. Ladd ⁎, Barbara S. McCrady, Jennifer K. Manuel, William Campbell
Center on Alcoholism, Substance Abuse, and Addictions (CASAA), 2650 Yale Blvd SE, MSC11-6280, Albuquerque, NM 87106, United States

a r t i c l e

i n f o

Keywords:
RCT
CONSORT
Alcohol treatment outcome

a b s t r a c t
It is critical that the reporting of randomized controlled trials (RCTs) be transparent and comprehensive. The
aim of this study was to examine if adopting standards of reporting, the Consolidated Standards of Reporting
Trials (CONSORT), improved the quality of reporting of alcohol treatment outcome studies. RCTs were
identiﬁed from eight journals publishing a substantial number of alcohol treatment outcome studies (n = <n>127</n>
RCTs) and coded for the quality of reporting according to the CONSORT guidelines. Both CONSORT adopter and
non-adopter journals showed signiﬁcant improvements in the quality of reporting of alcohol treatment
outcome studies over time. While overall results suggested a non-signiﬁcant trend for more improvement over
time in the quality of reporting for adopter compared to non-adopter journals, comparison of effects sizes
suggested that speciﬁc areas of reporting did signiﬁcantly improve for the adopter journals. Results suggest
that efforts to improve reporting such as the CONSORT guidelines can be useful and inﬂuential.
© 2010 Elsevier Ltd. All rights reserved.

1. Introduction
When thoughtfully developed and properly implemented, randomized controlled trials (RCTs) are the “gold standard” of research because
they provide researchers and clinicians with the best evidence of the
effectiveness and efﬁcaciousness of health-care interventions (Altman
et al., 2001). Unfortunately, signiﬁcant concerns too often arise about
the quality of a RCT. Such concerns may pertain to the study design and/
or to the reporting of the study, as poorly designed RCTs may present
misleading results (Altman et al., 2001; Schulz, Chalmers, Hayes, &
Altman, 1995). Within alcohol treatment outcome studies, this fact has
been acknowledged for more than forty years (Hill & Blane, 1967), and
multiple studies have evaluated the quality of alcohol treatment
outcome methodology (Breslin, Sobell, Sobell, & Sobell, 1997; Moyer,
Finney, & Swearingen, 2002; Sobell & Sobell, 1981). Several sources of
bias in alcohol RCT design, including pre-selection bias, differential
attrition rates, and under-powered tests, have been highlighted in
regard to their inﬂuence on estimated treatment effects (Finney, Hahn,
& Moos, 1996; Moyer, Finney, Elworth, & Kraemer, 2001). While the
methodological issues and gaps regarding alcohol RCTs are of concern,
recent reviews have found that there has been improvement in the
quality of alcohol RCTs over the past few decades (Moyer et al., 2002).
The quality of the reporting itself of RCTs is an issue inseparably
linked to the issue of RCT design, since the report is almost always the
only way for a reader to evaluate the quality of a study. No matter how
well-designed and conducted a RCT is, if the quality of the report is poor,
⁎ Corresponding author. Tel.: + 1 505 925 2333; fax: + 1 505 925 2301.
E-mail address: bladd@unm.edu (B.O. Ladd).
0306-4603/$ – see front matter © 2010 Elsevier Ltd. All rights reserved.
doi:10.1016/j.addbeh.2010.02.009

its impact will be attenuated due to the difﬁculties readers will have
understanding and evaluating the ﬁndings. In contrast, high quality
reporting of a RCT illuminates the ﬁndings and the implications of those
ﬁndings, as well as the potential sources of bias inherent in any
experimental design. Unfortunately, issues regarding the quality of
reporting have not received as much attention as concerns with
methodological quality in the alcohol treatment outcome literature.
Until recently, there were few, if any, agreed upon standards for
authors to follow when reporting RCTs; instead there were various
suggestions from multiple sources (Moher, 1998). Thus, the Consolidated Standards of Reporting Trials (CONSORT) Group was created to
improve the quality of the scientiﬁc literature based on RCTs by
developing standard guidelines (Begg et al., 1996). The CONSORT
statement was designed to guide authors in ensuring the transparency
of their reports, and to help the reader understand the design, conduct,
analysis and interpretation of a study. Since its initial publication, the
CONSORT statement has undergone several modiﬁcations and revisions.
The CONSORT statement lists speciﬁc dimensions for reporting RCTs,
and consists of a 22-item checklist and ﬂow diagram detailing how a
study was designed, analyzed, and interpreted. It was not developed as a
research quality assessment measure; rather, items address the internal
and external validity of a study. The items on the checklist fall into
several categories. The Introduction consists of items relevant to the
study rationale, as well as items on the objectives and hypotheses of the
study. Methods consists of items on the population being studied, the
measures being used, and the randomization process. Results includes
multiple items about participant recruitment and ﬂow and statistical
analyses. Discussion includes items about interpretation of the ﬁndings,
as well as limitations and generalizability. Since its initial publication in

B.O. Ladd et al. / Addictive Behaviors 35 (2010) 660–666

1996, the CONSORT statement has been lauded and accepted by many
academic journals, most of which are in the medical ﬁeld. Relatively few
publish psychosocial research studies.
The CONSORT statement does not provide standards for conducting
a RCT, but rather provides standards for the reporting of such a trial.
Nonetheless, a high quality report provides a clear description of the
quality of a study, and so allows the reader to quickly ascertain the
quality of the research itself. Thus, the CONSORT guidelines, in detailing
explicit standards for reporting, imply certain standards with regards to
the conduct of an RCT.
Studies have been conducted to test whether the CONSORT
statement has improved the quality of RCT reporting. In two studies
comparing journals that adopted the statement (“adopter” journals) to
non-adopter journals over time, it was found that the use of the
CONSORT statement was associated with improved quality of RCT
reporting (Kane, Wan, & Garrard, 2007; Moher, Jones, & Lepage, 2001).
Typically, adopter status is deﬁned by whether a journal includes
directions to follow CONSORT guidelines in the instructions to its
authors. In the most comprehensive analysis of the effectiveness of the
CONSORT statement to date, Plint et al. (2006) reviewed and
summarized eight studies published through 2005 that compared
CONSORT adopters to non-adopters, adopters before and after CONSORT publication, or some combination of the two. Their ﬁndings
suggested that improvement in the quality of reporting RCTs occurs
with the adoption of the CONSORT statement by a scientiﬁc journal.
Thus, when utilized, the CONSORT statement appears to be exerting
the originally intended effect on the quality of reporting of RCTs.
However, the CONSORT statement was created mainly with medical
clinical trials in mind; as such it has been evaluated primarily in medical
journals. For example, all eight studies reviewed in Plint et al. (2006), as
well as three additional studies (Balasubramanian et al., 2006; Piggott,
McGee, & Feuer, 2004; Tulikangas, Ayers, & O'Sullivan, 2006), examined
reporting in medical journals. Thus, dissemination and adoption of the
CONSORT statement largely has been the concern of medical journals, is
less often encountered in behavioral and psychological journals, and
none of the existing reviews of implementation of the CONSORT
guidelines included RCTs of psychosocial alcohol treatments.
In so far as the alcohol treatment outcome research ﬁeld relies on the
evidence provided from RCTs, it is important to assess the quality of
their reporting. Thus, the present study examined the current quality of
reporting based on the recommendations of the CONSORT statement.
An additional goal of this study was to examine the present state of
reporting of alcohol outcome studies and the implications of these
ﬁndings for future treatment research. Studies published in CONSORT
adopting journals were compared to studies published in non-adopting
journals to investigate the effects of the CONSORT statement on the
quality of RCT reporting. It was predicted that adopting journals would
show greater improvements in RCT reporting quality over time
compared to non-adopting journals.
2. Method
2.1. Journal selection
Journals publishing a substantial number of psychosocial alcohol
treatment outcome studies were identiﬁed using a guide from the
International Society of Addiction Journal Editors (Babor, Morisano, &
Stenius, 2004). The following four journals were the only journals
publishing addictions-related RCTs that were identiﬁed as having
adopted the CONSORT guidelines: 1) Addiction, 2) Alcohol and
Alcoholism, 3) Drug and Alcohol Dependence, and 4) Journal of Consulting
and Clinical Psychology. Adoption status was deﬁned by whether or not
the journal mentioned following the CONSORT statement in their
instructions to the authors. After identifying the adopter journals, four
non-adopter journals were matched to the adopter journals based on
their Institute of Scientiﬁc Information (ISI) impact factor and rate of

661

publishing alcohol treatment outcome studies. The selected nonadopters were: 1) Alcoholism: Clinical and Experimental Research, 2)
Journal of Studies on Alcohol and Drugs, 3) Journal of Substance Abuse
Treatment, and 4) Psychology of Addictive Behaviors. The mean (SD) ISI
impact factors for adopter and non-adopter journals were 3.37 (.92) and
2.49 (.48), respectively. Adopter journals did not signiﬁcantly differ
from non-adopter journals on impact factor, F (1, 6)= 3.02, ns.
2.2. Study selection
Two timeframes were selected from which studies were drawn:
1994–1998 (pre-adoption) and 2004–2008 (post-adoption). The preadoption timeframe was selected because the CONSORT statement was
ﬁrst published in the later part of 1996 and publication lag is commonly
around 12 months from ﬁrst submission of a manuscript. Thus, it is
likely that articles being published through 1998 were not being
impacted by the CONSORT statement. Additionally, none of the journals
in this study adopted the CONSORT statement immediately upon its
publication. The post-adoption timeframe was selected by starting from
the time we initiated the study and going back four years to match the
duration of the pre-adoption timeframe. In cases where journals had not
adopted the CONSORT guidelines prior to the identiﬁed time period
(two journals adopted in 2004), a one-year buffer period was given for
published studies. In other words, for the two journals that adopted the
CONSORT guidelines in 2004, studies were not included unless they
were published in 2005 or later to minimize the likelihood of their being
in the publication pipeline prior to the journal's adoption. Studies were
included if they: (1) evaluated at least one psychosocial treatment for
alcohol use disorders or risky drinking, (2) included a comparison
group, (3) used a randomization procedure designed to yield equivalent
groups, (4) evaluated at least one drinking outcome/consequences
measure, and (5) were not duplicate reports of a study that had already
appeared in one of the selected journals. The primary author (BL)
selected studies through table of content searches of the eight preselected journals during the speciﬁed time periods; yielding 127 eligible
articles (see Table 1 for the breakdown of studies by journal).
2.3. Measure
A 36-item coding manual was developed based on the CONSORT
checklist. The expanded items were arrived by modifying the CONSORT
checklist from a previous review study (Moher et al., 2001) and
incorporating adaptations relevant to psychosocial RCTs. The expansion
and elaboration of the 22-item CONSORT checklist is consistent with the
detailed explanation of the content of the CONSORT statement (Altman
et al., 2001). Each item could be assigned either a yes or no response.
Seven items also could be coded “not applicable.” As the CONSORT
statement was originally developed with medical clinical trials in mind,
it was decided that certain items of the coding manual might not apply
based on the study design (for example: if a study compared two
psychosocial treatments, it would not be possible to blind therapists to
treatment condition). Thus, it would not be appropriate to score that
item negatively (“no”), nor positively (“yes”) and “not applicable” was
provided as an alternative code. Items were organized by headings and
subheadings based on where the content pertaining to each item
typically was encountered in a manuscript (see Appendix A for the
coding record). However, the order of items was simply a guide — as
long as the content of an item was properly reported it was coded as yes,
regardless of where the reporting occurred in the manuscript. Earlier
versions of the coding manual were pilot tested by the authors and
issues were resolved through discussion and consensus.
2.4. Procedure
Once the coding manual was ﬁnalized, the sample of 127 studies was
listed alphabetically by ﬁrst author. The ﬁrst ﬁve articles were selected

662

B.O. Ladd et al. / Addictive Behaviors 35 (2010) 660–666

Table 1
Selected journals and number of published studies by time period.
Journal

Number of studies, Number of studies,
pre-adoption
post-adoption

Addiction
Alcohol and Alcoholism
Drug and Alcohol Dependence
Journal of Consulting and Clinical
Total number of studies, adopter
Alcoholism: Clinical and Experimental
Research
Journal of Studies on Alcohol and Drugs
Journal of Substance Abuse Treatment
Psychology of Addictive Behaviors
Total number of studies, non-adopter

10
4
0
10
24
8

3
15
8
9
35
14

6
1
2
17

19
8
10
51

for coder training. Four coders (the four authors) coded the articles for
this study. Twenty percent of studies were randomly selected to be
double-coded throughout the coding process. A random number
generator was used to select 23 studies for double coding. Coders
were then randomly assigned to each study. Once assignment of studies
to coders had been completed, each coder was provided with copies of
the full articles for each assigned study.
Coders met at four equal time points throughout the coding process
to assess reliability and to prevent coder drift. It was not feasible to mask
year published and author due to high rates of self-citation and dates in
reference lists. However, names of the source journals for each article
were concealed from coders. To check the success of masking, coders
were asked to guess the journal and adopter status of a portion of their
articles. Coders' guesses as to adopter status were not signiﬁcantly
better than chance (κ = .045, ns).
To assess the agreement among coders, ICCs were computed for each
coder pair. As the ﬁrst ﬁve articles were coded by all four coders and
were for training purposes, they were not included in the ﬁnal reliability
analyses. Each coder pair double-coded three to six studies. Using the
guidelines set by Cicchetti (1994), one coder pair had good reliability
(ICC= .739) and the remaining ﬁve coder pairs had excellent reliability
(ICCs ranged from .778 to .991). Percent agreement at the item level also
was found to be acceptable. Percent agreement ranged from 56.5% to
100%, with 14 of the 36 items having a percent agreement of 90% or
higher (see Appendix A). For studies that were double-coded, one
coder's ratings were selected randomly for use in the ﬁnal analyses.

selected as: (1) a repeated measures design was not appropriate, because
even though studies were rated at two time points for each journal, the
unit of analysis was at the study level and different studies were published
at time 1 or time 2, (2) a MANOVA framework was not appropriate as
time was not a truly independent variable. Thus, comparison of effect
sizes was selected as the strongest method for detecting differences in
change over time. ICC and ANOVA analyses were conducted using SPSS
v12 software (SPSS Inc., 2003) and effect sizes were calculated and
compared using Ralf Schwarzer's Meta-Analysis Program v5.3.
3. Results
3.1. Pre-CONSORT comparison of adopter versus non-adopter journals
Table 1 provides the breakdown of number of articles selected from
each journal by time period.
Studies published pre-CONSORT (1994–1998) did not differ
signiﬁcantly on overall CONSORT score between adopter and nonadopter journals, F (1, 39) = .747, ns. Nor did studies signiﬁcantly
differ on any of the aggregate subscores: random assignment score,
F (1, 39) = 1.595, ns, masking score, F (1, 39) = .145, ns, participant
ﬂow score, F (1, 39) = .044, ns, or statistics score, F (1, 39) = .615, ns.
Thus, studies published prior to the existence of the CONSORT
statement did not differ signiﬁcantly in their quality of reporting
based on whether the journal later endorsed the CONSORT statement.
See Table 2 for overall mean scores and scores by aggregate subscore.
3.2. Changes in quality of reporting over time (Table 2)
Both adopter and non-adopter journals showed signiﬁcant improvements in the overall quality of reporting over time, F (1, 57)=23.72,
pb .001 and F (1, 66)=7.82, pb .01, respectively. However, there were
different results depending on adopter status for the aggregate subscores.
For adopter journals, signiﬁcant improvements over time were observed
for random assignment, F (1, 57)=8.10, pb .01, masking, F (1, 57)=
10.64, pb .01, participant ﬂow, F (1, 57)=8.39, pb .01, and statistics,
F (1, 57)=6.92, pb .05. For non-adopter journals, a non-signiﬁcant
trend suggested some improvement over time for random assignment,
F (1, 66)=3.55, p=.06. Results for masking, F (1, 66)=.32, ns, participant ﬂow, F (1, 66)=.24, ns, and statistics, F (1, 66)=1.74, ns, were all
non-signiﬁcant.
3.3. Comparison of effect sizes

2.5. Data analysis
Data were double entered and compared to assure accuracy. Based on
a priori conceptual groupings, four aggregate subscores were computed.
These groupings were selected to focus speciﬁcally on the quality of
reporting of methodology and results, which are considered as of central
importance in reporting of alcohol outcome studies. The aggregate
subscores contained items pertaining to: (1) random assignment, (2)
masking, (3) participant ﬂow through the stages of the study, and (4)
statistical analyses and results. For each grouping, selected items were
tallied for the number of yes responses and then divided by the total
number of items in the grouping to provide a score that represented
percent compliance with the CONSORT reporting standard. “Not
Applicable” codes were treated as missing data (see Appendix A for
the speciﬁc items that compromised each aggregate subscore). An
overall score for all items also was computed in this manner.
A three step data analysis plan was followed. First, a one-way ANOVA
was computed to examine whether adopter journals differed from nonadopter journals at time 1 (pre-CONSORT). Next, ANOVAs were
computed separately for adopter and non-adopter journals over time.
Finally, effect sizes for each aggregate subscore were calculated for
adopters and non-adopters. Lastly, the effect sizes were tested to
determine whether they differed signiﬁcantly. This analysis plan was

To compare the results for adopter and non-adopter journals, prepost effect sizes were computed for overall score and for the four
aggregate subgroups (see Fig. 1).
An unprotected Fisher's exact test suggested a non-signiﬁcant trend
for improvement over time in the reporting for adopter compared to
non-adopter journals (p = .07). Additional unprotected tests showed
that change over time for aggregate subscores for random assignment
and statistics items for adopter journals did not signiﬁcantly differ from
those of the non-adopter journals. The comparison of effect sizes for
reporting of masking was signiﬁcantly different; adopter journals
improved more than did the non-adopter journals (p b .05). The effect
size for participant ﬂow also was found to be signiﬁcantly greater in
adopter journals than non-adopter journals (p b .01).
3.4. Descriptive data on general quality of reporting
In addition to examining the improvements in quality of reporting
both over time and by adopter status, it was of interest to assess the
overall frequency with which items of the CONSORT statement were
adequately reported in the alcohol outcome literature. The frequency
with which each individual item was reported is provided in Appendix A.
Ten of the 36 items were adequately reported over 90% of the time (items

B.O. Ladd et al. / Addictive Behaviors 35 (2010) 660–666
Table 2
Mean CONSORT scores overall and by aggregate subgroup.
Adopters (mean, SD)
Overall (36 items)
Pre-adoption
20.83
Post-adoption
24.43
Random assignment (5 items)
Pre-adoption
2.25
Post-adoption
3.43
Masking (2 items)
Pre-adoption
.12
Post-adoption
.54
Participant ﬂow (3 items)
Pre-adoption
2.00
Post-adoption
2.63
Statistics (7 items)
Pre-adoption
5.42
Post-adoption
6.17

Non-adopters (mean, SD)

(4.58)
(3.27)*

19.53 (3.81)
22.14 (3.41)*

(1.39)
(1.67)*

1.71 (1.31)
2.37 (1.25)

(.34)
(.56)*

.18 (.53)
.25 (.48)

(.93)
(.73)*

1.94 (.83)
1.78 (1.22)

(1.32)
(.89)*

5.06 (1.60)
5.57 (1.30)

Note: Scores represent number of items assigned the code of yes. * indicates p b .05 for
comparison of pre- versus post-scores.

2, 3, 4, 7, 9, 10, 12, 19, 33, and 36). Another seven items were properly
reported more than three-quarters of the time (items 8, 18, 26, 28, 29, 30,
and 34). Items 5, 20, 23, 24, 25, 27, and 35 were all reported in more than
half of the time. Thus, 67% (24 of 36) items were reported in over half of
the 127 studies reviewed. Of the remaining twelve items, only ﬁve (items
11, 17, 22, 31, and 32) were reported in less than 20% of studies.
4. Discussion
Overall, the quality of reporting of alcohol treatment outcomes was
found to have improved over time. The improvement in reporting
mirrors improvements in the conduct of alcohol treatment outcome
studies (Breslin et al., 1997; Moyer et al., 2002). Of note, the results
suggest journals that adopted the CONSORT statement appear to have
improved to a greater degree on some key domains. This indicates that
the quality of reporting is getting better, and that efforts to improve
reporting, such as the CONSORT guidelines, can be useful and inﬂuential.
Moreover, the improvements observed in reports published in
journals adopting the CONSORT statement address many of the critical
issues that have been identiﬁed in methodological reviews of the alcohol
outcome literature. Signiﬁcant improvements by adopter journals were
observed in reporting of participant recruitment and retention numbers

Fig. 1. Pre-post effect sizes by adopter status overall and each aggregate subgroup.

663

throughout the study. This trend is heartening, as it allows for better
evaluation of selection bias, which has been found to inﬂuence alcohol
treatment outcomes (Finney et al., 1996). Failure to take differential
attrition rates into account can affect interpretation of outcomes (Moyer
et al., 2002). Likewise, adequate reporting of statistical methods and
results showed signiﬁcant improvements, allowing readers to better
assess and interpret treatment effects and generalizability of results.
A number of factors likely contributed to improvements in study
reporting over time. Several review papers (e.g. Breslin et al., 1997;
Moyer et al., 2001; Moyer et al., 2002) published in the past 15 years
highlighted strengths and weaknesses in alcohol treatment outcome
methods. Project MATCH was conducted during this time, and set a new
and higher standard for the conduct of RCTs in the alcohol ﬁeld
(Donovan & Mattson, 1994). In the U.S., the American Psychological
Association developed a Task Force on Empirically Supported Treatments (EST) (Task Force, 1995), which speciﬁed research design criteria
for a treatment to be considered an EST, and the U.S. Substance Abuse
and Mental Health Services Administration also deﬁned research
criteria to list treatments on their National Registry of Evidence-Based
Programs and Practices (National Registry of Evidence-Based Programs
& Practices, 2009). The international Cochrane Collaboration, established in 1993, has been conducting and publishing reviews of evidencebased health care (The Cochrane Collaboration, 2009). In the UK, the
National Institute for Health and Clinical Excellence (NICE) (National
Institute for Health & Clinical Evidence, 2009) has been reviewing
empirical literature to establish guidelines for health care. The research
reports rated for the present study were conducted within the context of
these multiple inﬂuences on the design and reporting of RCTs.
Moving past the comparison of CONSORT adopting journals versus
non-adopting journals, certain areas of reporting were stronger than
others (see Appendix A for frequencies with which individual items were
reported). Reports identiﬁed studies as RCTs in the abstract, covered the
literature and set out their objectives (items 2–4) in more than 90% of the
studies reviewed. Reports of alcohol RCTs also were good at describing
their study populations, interventions, outcome measures and statistical
analyses (items 7–10 and 18–19) and describing the study implications
and limitations (items 33–34 and 36). However, there also were areas of
reporting that were relatively weaker. While authors often identiﬁed
their study as randomized in the abstract, only a third of reports
identiﬁed the study as randomized in the title (item 1). This omission is
important, especially with the increasing reliance on electronic search
techniques. Reports rarely described the procedure used to determine
sample size (item 11). Authors often failed to report the details of their
randomization procedure (items 13–15). This ﬁnding is problematic
since many sources of bias can arise when the randomization process is
poorly implemented, and randomization is a cornerstone of RCTs. Few
studies addressed the issue of masking (items 16–17), although this
omission may have occurred because the CONSORT statement was
developed with studies of medical treatments in mind and masking is
less relevant to certain aspects of psychosocial treatment studies.
The issue of reporting power in alcohol RCTs is especially troubling,
as some authors have noted (e.g. Moyer et al., 2002) a sizable number of
studies being conducted were done so with inadequate power to detect
the presence of a treatment effect. Although some might argue that
power analyses are redundant in a study that reports a signiﬁcant
treatment effect, from a methodological standpoint it can be argued that
when researchers fail to report their power analyses, reviewers cannot
be sure how clearly the intervention has been conceptualized. Knowing
how researchers have conceptualized the effect size of their treatment is
as informative in its own way as knowing the theory upon which the
intervention is based. There are also good ethical reasons for reporting
power analyses. Increasing the number of subjects in a study can
increase power, and conducting a power analyses precludes a
researcher from running subjects until chance affords them a favourable
outcome. Finally, lacking a power analysis, it is difﬁcult to interpret
mixed or null results, since doing so entails both the speciﬁcation of

664

B.O. Ladd et al. / Addictive Behaviors 35 (2010) 660–666

effect size, rationale around corresponding signiﬁcance levels, and
clariﬁcation of other related statistical assumptions that might be
contributing to loss of power as well. Thus, especially when considering
that many interventions in the alcohol treatment literature carry at best
modest effect sizes, reporting power analyses compels researchers to
provide clearer rationales and tighter methodologies for their research.
The strengths of the present study include the large sample size of
studies reviewed. This study used all 127 randomized controlled trials
meeting study inclusion, yielding good power to detect signiﬁcant
effects. Another strength was the careful development and monitoring
of the rating system. The coding manual went through multiple
revisions prior to coding based on coder feedback and consensus
meetings of the authors regarding which items were clearly deﬁned and
which items needed further clariﬁcation. Additionally, coder reliability
was assessed carefully throughout the study.
There are also some weaknesses in the present study. First, coders
varied in their familiarity with the body of literature under review. One
of the coders is an experienced alcohol researcher and the others are
graduate students with interests and various levels of experience in
alcohol research. As such, the coders may have had knowledge of some
of the studies prior to this study. While efforts to control this potential
source of bias were undertaken (masking to journal of publication,
random assignment of studies), it is possible that this familiarity with
the literature could have affected some of the ratings.
Another potential weakness of this study regards the difﬁculty the
coders encountered in rating speciﬁc items. Even after discussion and
multiple revisions of the coding manual, select items continued to be
troublesome in terms of reliable coding. While this may be a weakness
of the current study, it also points to one of the difﬁcult issues of
reporting RCTs. Authors are under tight length restrictions and have to
explain complicated designs. For example, a thoughtful description of a
power analysis can be lengthy and may be omitted from a report to save
space. Additionally, there are some items that were impossible to fully
objectify, leaving room for subjective interpretation. For example, rating
whether or not descriptive statistics were reported appropriately (item
28) was inherently a subjective judgment.
One potential limitation of adopting the CONSORT statement for
reports involving psychosocial interventions stems from the fact that
the CONSORT statement was developed largely for medical trials, where
RCTs commonly involve testing biological treatments, such as pharmacological interventions or surgical procedures. In psychosocial research,
some of the design issues included in the CONSORT are less
straightforward, such as masking and multi-component interventions
(Boutron, Moher, Altman, Schulz, & Ravaud, 2008). In addition, some
important aspects of psychosocial RCTs are not addressed in the
CONSORT statement. An example is the adequate reporting of treatment
integrity. Treatment integrity may be a less complex issue for
pharmacological studies that can use pill counts, but in psychosocial
studies it is more challenging to determine whether therapists were
consistent in providing the speciﬁed intervention. Fortunately, issues
around the unique methodological issues that pertain to psychosocial
RCTs have been receiving attention more recently, and the CONSORT
Group has created of an extension of the CONSORT statement for
reporting RCTs of non-pharmacologic treatments (NPT) (Boutron et al.,
2008). These new CONSORT standards are likely to lead to further
improvements in the reporting of psychosocial RCTs.
Another possible limiting factor is that the extent to which journals
require their authors to adhere to the CONSORT guidelines was not
controlled for. In a recent study examining high impact factor medical
journals, a range of ambiguity was found regarding the instructions to
authors on adhering to the CONSORT guidelines (Hopewell, Altman,
Moher, & Schulz, 2008). Of 62 journals including mention of the
CONSORT statement in their ‘instructions to authors’, only 23, or about a
third, state CONSORT adherence is “required.” In this same study, only
47% of surveyed editors reported that their journal incorporates
CONSORT adherence into the editorial process, and only 41% reported

incorporating it into the peer-review process. In a brief survey collected
after data collection of the editors of the four adopter journals, a range of
required adherence was found. One journal simply had a statement
about the CONSORT statement on its website. Another journal had a
statement regarding the CONSORT guidelines on its website, and
occasionally incorporated the CONSORT statement into its editorial
process, although this was not practiced systematically. Two journals
had incorporated the CONSORT statements into the editorial review
process, and one of those journals also required authors to submit a
completed CONSORT checklist. Thus, the journals identiﬁed as adopters
in this study varied quite a bit on the extent to which the CONSORT
statement was adhered to and incorporated into the review process.
Along these lines, while authors were not explicitly directed towards the
CONSORT guidelines in the non-adopter journals, it is possible that
speciﬁc editors and/or reviewers still used the CONSORT statement in
some capacity during the review process. This potential variability of
implementation of the CONSORT guidelines across both adopter and
non-adopter journals could limit the ﬁndings of this study.
While improving the reporting of RCTs does not appear to directly
affect how research is conducted, addressing shortcomings in how
studies are reported does have implications for the conduct of research.
By establishing standards of reporting such as the CONSORT statement, a
standard for conducting RCTs is implied. Standards for quality reporting
also provide a method for easily discerning whether a study was
conducted in a way that maximizes the integrity and interpretation of
the results. If standards for reporting become more widely disseminated
and accepted, it is likely that a feedback effect will occur, whereby
investigators will begin to incorporate the identiﬁed CONSORT reporting elements into their protocols and experimental designs (for
example, keeping better records of drop-out rates with the knowledge
that such information will be expected when it comes time to publish
one's results).
The results of the present study are promising. Not only has the
quality of reporting of alcohol treatment outcome studies improved
over time, but it also appears that researchers can facilitate further
improvement by establishing standards and guidelines for reporting
RCTs. In this way, the research being conducted and disseminated can be
evaluated easily and thoroughly. Studies of poorer quality can be
identiﬁed readily and the strength of studies with apparently
groundbreaking ﬁndings can be assessed critically. There is still room
for improvement in the reporting of alcohol RCTs. Certain areas, such as
reporting of randomization and masking procedures, require special
attention. By establishing and promoting standards and guidelines for
high quality reporting, the ﬁeld will be able to identify and resolve weak
areas of reporting while strengthening strong areas.
Role of Funding Sources
Financial support for this study was provided by the Center on Alcoholism, Substance
Abuse, and Addictions (CASAA) at the University of New Mexico.

Contributors
Neither CASAA nor UNM had any involvement in the conduct of this research. All
authors participated in study design, data collection, and contributed to the ﬁnal
manuscript. In addition, Benjamin Ladd conducted literature searches, ran the
statistical analyses, and wrote the ﬁrst draft of the manuscript.

Conﬂict of Interest
My co-authors and I do not represent any interests that could be interpreted as
inﬂuential in this research. All relevant APA ethical standards were followed. I testify to
the accuracy of the above on behalf of all the authors.

Acknowledgements
The authors would like to thank Scott Tonigan for his insight and advice regarding
the statistical analyses of this study. We would also like to thank Jon Houck for his help
with the randomization process and reliability tests, as well as David Moher for
providing his coding template from which the coding manual was developed.

B.O. Ladd et al. / Addictive Behaviors 35 (2010) 660–666

665

Appendix A
Sample coding record with coding frequencies and percentage agreement between coders at the item level.
Heading

Item

Descriptor

Yes

Title
Abstract
Introduction

1
2
3
4
5
6

Identify study as a randomized trial
Identify study as a randomized trial and use a structured format
Scientiﬁc background and explanation of rationale for study
State objectives
State deﬁned hypotheses
State planned subgroup or covariate analyses

42
115
125
118
80
38

85
12
2
9
47
12

7
8
9
10
11
12a
13a
14a
15a
16b
17b
18d
19d
20d
21
22

Planned study population
Inclusion and exclusion criteria
Planned interventions and their timing
Primary and secondary outcome measure(s)
Procedure for projecting target sample size (e.g. power analysis)
Unit of randomization (e.g. individual, cluster, etc.)
Method used to generate the randomization schedule
Method of randomization concealment
Method to separate the generator from the executor of assignment
Describe method of blinding (if blinding warranted or even possible)
Describe evidence for success of blinding procedure (again, if possible)
Rationale for statistical analyses
Methods of statistical analyses
Detailed main analyses and whether completed on an intention-to-treat basis
Detailed subgroup or post hoc analyses
Deﬁne stopping rules (if warranted)

122
109
124
123
21
118
58
27
34
35
3
112
123
86
47
0

5
18
3
4
106
9
69
100
93
89
32
15
4
41
8
0

23c
24a
25c
26c
27d

Provide trial proﬁle (ﬂowchart) summarizing ns for each randomized group
Provide trial proﬁle (ﬂowchart) summarizing timing of randomization assignment
Provide trial proﬁle (ﬂowchart) summarizing the interventions for each randomized group
Provide trial proﬁle (ﬂowchart) summarizing the measurements for each randomized group
State estimated effect of intervention on primary and secondary outcomes measure(s), including measure
of precision
State results in absolute numbers where feasible (e.g. 50 out of 100, not 50%)
Present appropriate descriptive statistics in enough detail to permit alternative analyses and/or replication
Present appropriate inferential statistics in enough detail to permit alternative analyses and/or replication
Prognostic variables by treatment group and any attempts to adjust for them
Describe protocol deviations from planned study, and the reasons for such deviations
State speciﬁc interpretation of study ﬁndings
State limitations of study, including potential sources of bias and imprecision (internal validity)
State discussion of external validity, including appropriate quantitative measures where possible
State general interpretation of the data in light of currently available evidence

82
89
86
96
77

45
38
41
31
50

78.3
65.2
73.9
73.9
65.2

97
109
113
10
6
127
106
70
123

30
18
14
3
1
0
21
57
4

69.6
91.3
82.6
69.6
87.0
100
95.7
65.2
95.7

Methods
Protocol

Assignment

Masking (blinding)
Statistical analyses

Results
Participant ﬂow and
follow-up

Analysis

Discussion

a
b
c
d

28d
29d
30d
31
32
33
34
35
36

No

N/A

77

3
92

72
127

114
120

Coder%
agree
100.0
100.0
95.7
91.3
87.0
69.6
87.0
82.6
100
100
91.3
82.6
82.6
78.3
82.6
78.3
78.3
91.3
95.7
69.6
56.5
100

Items in the random assignment conceptual group.
Items in the masking conceptual group.
Items in the statistical analyses and results conceptual group.
Items in the participant ﬂow conceptual group.

References
Altman, D. G., Schulz, K. F., Moher, D., Egger, M., Davidoff, F., Elbourne, D., et al. (2001).
The Revised CONSORT statement for reporting randomized trials: Explanation and
elaboration. Annals of Internal Medicine, 134, 663−694.
Babor, T. F., Morisano, D., & Stenius, K. (2004). How to choose a journal: Scientiﬁc and
practical considerations. In T. F. Babor, K. Stenius, & S. Savva (Eds.), Publishing
addiction science: A guide for the perplexed. Rockville, MD: Social and Health
Services, Ltd.
Balasubramanian, S. P., Weiner, M., Alshameeri, Z., Tiruvoipati, R., Elbourne, D., & Reed,
M. W. (2006). Standards of reporting of randomized controlled trials in general
surgery: Can we do better? Annals of Surgery, 244, 663−667.
Begg, C. B., Cho, M. K., Eastwood, S., Horton, R., Moher, D., Olkin, I., et al. (1996).
Improving the quality of reporting of randomized controlled trials: The CONSORT
statement. JAMA, 276, 637−639.
Boutron, I., Moher, D., Altman, D. G., Schulz, K., & Ravaud, P. (2008). Extending the
CONSORT Statement to randomized trials of nonpharmacologic treatment:
Explanation and elaboration. Annals of Internal Medicine, 148, 295−309.
Breslin, F. C., Sobell, S. L., Sobell, L. C., & Sobell, M. B. (1997). Alcohol treatment outcome
methodology: State of the art 1989–1993. Addictive Behaviors, 22, 145−155.
Cicchetti, D. V. (1994). Guidelines, criteria, and rules of thumb for evaluating normed
and standardized assessment instruments in psychology. Psychological Assessment,
6, 284−290.
Donovan, D. M., & Mattson, M. E. (1994). Alcoholism treatment matching research:
Methodological and clinical approaches. Journal of Studies on Alcohol, Supplement,
5−14.
Finney, J. W., Hahn, A. C., & Moos, R. H. (1996). The effectiveness of inpatient and
outpatient treatment for alcohol abuse: The need to focus on mediators and
moderators of setting effects. Addiction, 91, 1773−1796.

Hill, M. J., & Blane, H. T. (1967). Evaluation of psychotherapy with alcoholics: A critical
review. Quarterly Journal of Studies on Alcohol, 28, 76−104.
Hopewell, S., Altman, D. G., Moher, D., & Schulz, K. F. (2008). Endorsement of the
CONSORT Statement by high impact factor medical journals: A survey of journal
editors and journal ‘instructions to authors’. Trials, 9, 20.
Kane, R. L., Wan, J., & Garrard, J. (2007). Reporting in randomized clinical trials
improved after adoption of the CONSORT statement. Journal of Clinical Epidemiology, 60, 241−249.
Moher, D. (1998). CONSORT: An evolving tool to help improve the quality of reports of
randomized controlled trials. JAMA, 279, 1489−1491.
Moher, D., Jones, A., & Lepage, L. (2001). Use of the CONSORT statement and quality of
reports of randomized trials: A comparative before-and-after evaluation. JAMA,
285, 1992−1995.
Moyer, A., Finney, J. W., Elworth, J. T., & Kraemer, H. C. (2001). Can methodological
features account for patient-treatment matching ﬁndings in the alcohol ﬁeld?
Journal of Studies on Alcohol, 62, 62−73.
Moyer, A., Finney, J. W., & Swearingen, C. E. (2002). Methodological characteristics and
quality of alcohol treatment outcome studies, 1970–98: An expanded evaluation.
Addiction, 97, 253−263.
National Institute for Health and Clinical Evidence. http://www.nice.org.uk/. Cited
March 5, 2009.
National Registry of Evidence-Based Programs and Practices. http://www.nrepp.
samhsa.gov/index.asp. Cited March 5, 2009.
Piggott, M., McGee, H., & Feuer, D. (2004). Has CONSORT improved the reporting of
randomized controlled trials in the palliative care literature? A systematic review.
Palliative Medicine, 18, 32−38.
Plint, A. C., Moher, D., Morrison, A., Schulz, K., Altman, D. G., Hill, C., et al. (2006). Does
the CONSORT checklist improve the quality of reports of randomized controlled
trials? A systematic review. Medical Journal of Australia, 185, 263−267.

666

B.O. Ladd et al. / Addictive Behaviors 35 (2010) 660–666

Schulz, K. F., Chalmers, I., Hayes, R. J., & Altman, D. G. (1995). Empirical evidence of bias.
Dimensions of methodological quality associated with estimates of treatment
effects in controlled trials. JAMA, 273, 408−412.
Sobell, L. C., & Sobell, M. B. (1981). Outcome criteria and the assessment of alcohol
treatment efﬁcacy. Evaluation of the alcoholic: Implications for research, theory and
treatmentIn NIAAA (Ed.), Research monograph, vol. 5. (pp. 369−382) Rockville, MD:
MIAAA.

Task Force on Promotion and Dissemination of Psychological Procedures. (1995).
Training in and dissemination of empirically-validated psychological treatments:
Report and recommendations. The Clinical Psychologist, 48, 3−23.
The Cochrane Collaboration. http://www.cochrane.org/index.htm. Cited March 5, 2009.
Tulikangas, P. K., Ayers, A., & O'Sullivan, D. M. (2006). A meta-analysis comparing trials
of antimuscarinic medications funded by industry or not. BJU International, 98,
377−380.